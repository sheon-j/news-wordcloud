{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyON5YShAYV3K5YMumzuv+ug",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheon-j/news-wordcloud/blob/main/data/crawler/adult_news_crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 402,
      "metadata": {
        "id": "uynIuNZ0uRcg"
      },
      "outputs": [],
      "source": [
        "from urllib import request\n",
        "from urllib.parse import quote\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from datetime import datetime\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# crawl href\n",
        "def crawl_href(url):\n",
        "  req = request.urlopen(url).read()\n",
        "  soup = BeautifulSoup(req, 'html.parser')\n",
        "  \n",
        "  # get href from target news link\n",
        "  target_news_list=[]\n",
        "  for div in soup.select('.rankingnews_box'):\n",
        "    if div.select_one('.rankingnews_name').text in target_news:\n",
        "      target_news_list.append(div.select_one('.list_content > a').get('href'))\n",
        "\n",
        "  return target_news_list\n",
        "\n",
        "\n",
        "# crawl data\n",
        "def crawl_data(url):\n",
        "  req = request.urlopen(url).read()\n",
        "  soup = BeautifulSoup(req, 'html.parser')\n",
        "\n",
        "  # get data from target news data\n",
        "  json_data = {}\n",
        "  # news_url\n",
        "  json_data['news_url'] = url\n",
        "\n",
        "  # news_source\n",
        "  news_source = soup.select_one('.ofhd_float_title_text').text\n",
        "  json_data['news_source'] = news_source\n",
        "\n",
        "  # news_title\n",
        "  news_title = soup.select_one('.media_end_head_headline').text\n",
        "  json_data['news_title'] = news_title\n",
        "\n",
        "  # news_date\n",
        "  news_date = soup.select_one('.media_end_head_info_datestamp span').get('data-date-time')\n",
        "  json_data['news_date'] = news_date\n",
        "\n",
        "  # news_writer\n",
        "  if soup.select_one('.media_end_head_journalist_name'):\n",
        "    news_writer = soup.select_one('.media_end_head_journalist_name').text.split(\" \")[0]\n",
        "  else:\n",
        "    news_writer = ''\n",
        "  json_data['news_writer'] = news_writer\n",
        "\n",
        "  # news_subtitle\n",
        "  if soup.select_one('.media_end_summary'):\n",
        "    news_subtitle = soup.select_one('.media_end_summary').text\n",
        "  else:\n",
        "    news_subtitle = ''\n",
        "  json_data['news_subtitle'] = news_subtitle\n",
        "\n",
        "  # news_img\n",
        "  if soup.select_one('#img1'):\n",
        "    news_img = soup.select_one('#img1').get('data-src')\n",
        "  else:\n",
        "    news_img = ''\n",
        "  json_data['news_img'] = news_img\n",
        "\n",
        "  # news_article\n",
        "  # 이미지 설명 제거 (추후 연구)\n",
        "  news_article = ''.join([p for p in soup.select_one('#dic_area') \n",
        "                          if p.find('em.img_desc')])\n",
        "  # 공백문자 처리\n",
        "  news_article = re.sub('[\\xa0\\u200b\\n\\r\\t]|\\s{2,}', \"\", news_article).strip()\n",
        "  json_data['news_article'] = news_article\n",
        "\n",
        "  return json_data"
      ],
      "metadata": {
        "id": "vDksIkf_AYaI"
      },
      "execution_count": 403,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target: 대힌민국 10대 중앙 종합 일간지\n",
        "# 서울신문, 조선일보, 동아일보, 경향신문, 한국일보, 중앙일보, 한겨레, 국민일보, 세계일보, 문화일보\n",
        "target_news = [\n",
        "  '서울신문', '조선일보', '동아일보', '경향신문', '한국일보',\n",
        "  '중앙일보', '한겨레', '국민일보', '세계일보', '문화일보'\n",
        "]\n",
        "news_list = []\n",
        "data = []\n",
        "\n",
        "# 많이 본 뉴스 리스트\n",
        "url = \"https://news.naver.com/main/ranking/popularDay.naver?date=\"\n",
        "\n",
        "# 날짜 기간 지정\n",
        "date_list = pd.date_range('20220101', '20220228').strftime('%Y%m%d').tolist()"
      ],
      "metadata": {
        "id": "zYa7KvmDvBwO"
      },
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crawl href\n",
        "for date in date_list:\n",
        "  news_list += crawl_href(url + date)"
      ],
      "metadata": {
        "id": "BEwQ6dmXa5dk"
      },
      "execution_count": 405,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crawl news data\n",
        "for news in news_list:\n",
        "  data.append(crawl_data(news))"
      ],
      "metadata": {
        "id": "96LACJykceiD"
      },
      "execution_count": 406,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save to csv\n",
        "df_data = pd.DataFrame(data)\n",
        "df_data.to_csv('adult_news_data.csv',mode = 'w', index=False)"
      ],
      "metadata": {
        "id": "OATg70PudCc4"
      },
      "execution_count": 408,
      "outputs": []
    }
  ]
}